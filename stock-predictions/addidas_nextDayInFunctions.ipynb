{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Flo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline\n",
    "from numpy import array\n",
    "import datetime\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expected soemthing like this (e.g.): load_plot_to_tensorboard([range(len(train_data)),train_data,'k',\"simple presentation of orginal data\"], [same structue], ..., z)\n",
    "#expected something like this (e.g.): load_plot_to_tensorboard_x_plots_y_scatter()\n",
    "def load_plot_to_tensorboard_x_plots_y_scatter(plots=[],scatter=[],name=\"Not Specified\"):\n",
    "    plt.ioff()\n",
    "    plt.figure(figsize=(40, 15))\n",
    "    plt.title('stock market prediction - Prediction on Prediction')\n",
    "    for i in plots:\n",
    "        plt.plot(i[0],i[1],i[2])\n",
    "    for j in scatter:\n",
    "        plt.scatter(x=j[0],y=j[1],c=j[2]) \n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    image=tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    return tf.summary.image(name,image,max_outputs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list expected: [data_set,scaled_dataset,training_data,test_data]\n",
    "def create_and_add_summaries(data_list,writer=None,session=None):\n",
    "    color_original_data_plot='b'\n",
    "    color_train_data_plot='g'\n",
    "    color_original_data_scatter='blue'\n",
    "    color_train_data_scatter='green'\n",
    "    color_test_data_plot='m'\n",
    "    color_test_data_scatter=\"magenta\"\n",
    "    \n",
    "    summary_original_unscaled = session.run(load_plot_to_tensorboard_x_plots_y_scatter([[range(len(data_list[0])),data_list[0],color_original_data_plot]],name=\"UNSCALED_ORIGINAL_DATA_PLOT\"))\n",
    "    summary_original = session.run(load_plot_to_tensorboard_x_plots_y_scatter([[range(len(data_list[1])),data_list[1],color_original_data_plot]],name=\"ORIGINAL_DATA_PLOT\"))\n",
    "    summary_train = session.run(load_plot_to_tensorboard_x_plots_y_scatter([[range(window_size,window_size+len(data_list[2])),data_list[2],color_train_data_plot]],name=\"TRAIN_DATA_PLOT\"))\n",
    "    summary_original_with_train = session.run(load_plot_to_tensorboard_x_plots_y_scatter([[range(len(data_list[1])),data_list[1],color_original_data_plot],[range(window_size,window_size+len(data_list[2])),data_list[2],color_train_data_plot]],name=\"ORIGINAL_DATA_AND_TRAIN_DATA_PLOT\"))\n",
    "    summary_test = session.run(load_plot_to_tensorboard_x_plots_y_scatter([[range(window_size+train_range ,window_size+ train_range+ len(data_list[3])),data_list[3],color_test_data_plot]],name=\"TEST_DATA_PLOT\"))\n",
    "    summary_test_with_train = session.run(load_plot_to_tensorboard_x_plots_y_scatter([[range(window_size+train_range ,window_size+ train_range+ len(data_list[3])),data_list[3],color_test_data_plot],[range(window_size,window_size+len(data_list[2])),data_list[2],color_train_data_plot]],name=\"TEST_AND_TRAIN_DATA_PLOT\"))\n",
    "    summary_all = session.run(load_plot_to_tensorboard_x_plots_y_scatter([[range(len(data_list[1])),data_list[1],color_original_data_plot],[range(window_size,window_size+len(data_list[2])),data_list[2],color_train_data_plot],[range(window_size+train_range ,window_size+ train_range+ len(data_list[3])),data_list[3],color_test_data_plot]],name=\"SUMMARIZE_ALL_PLOT\"))\n",
    "    \n",
    "    summary_original_scatter = session.run(load_plot_to_tensorboard_x_plots_y_scatter([],[[range(len(data_list[1])),data_list[1],color_original_data_scatter]],name=\"ORIGINAL_DATA_SCATTER\"))\n",
    "    summary_train_scatter = session.run(load_plot_to_tensorboard_x_plots_y_scatter([],[[range(window_size,window_size+len(data_list[2])),data_list[2],color_train_data_scatter]],name=\"TRAIN_DATA_SCATTER\"))\n",
    "    summary_original_with_train_scatter = session.run(load_plot_to_tensorboard_x_plots_y_scatter([],[[range(len(data_list[1])),data_list[1],color_original_data_scatter],[range(window_size,window_size+len(data_list[2])),data_list[2],color_train_data_scatter]],name=\"ORIGINAL_DATA_AND_TRAIN_DATA_SCATTER\"))\n",
    "    summary_test_scatter = session.run(load_plot_to_tensorboard_x_plots_y_scatter([],[[range(window_size+train_range ,window_size+ train_range+ len(data_list[3])),data_list[3],color_test_data_scatter]],name=\"TEST_DATA_SCATTER\"))\n",
    "    summary_test_with_train_scatter = session.run(load_plot_to_tensorboard_x_plots_y_scatter([],[[range(window_size+train_range ,window_size+ train_range+ len(data_list[3])),data_list[3],color_test_data_scatter],[range(window_size,window_size+len(data_list[2])),data_list[2],color_train_data_scatter]],name=\"TEST_AND_TRAIN_DATA_SCATTER\"))\n",
    "    summary_all_scatter = session.run(load_plot_to_tensorboard_x_plots_y_scatter([],[[range(len(data_list[1])),data_list[1],color_original_data_scatter],[range(window_size,window_size+len(data_list[2])),data_list[2],color_train_data_scatter],[range(window_size+train_range ,window_size+ train_range+ len(data_list[3])),data_list[3],color_test_data_scatter]],name=\"SUMMARIZE_ALL_SCATTER\"))\n",
    "    \n",
    "    summary_list = [summary_original_unscaled,summary_original,summary_train,summary_original_with_train,summary_test,summary_test_with_train,summary_all,summary_original_scatter,summary_train_scatter,summary_original_with_train_scatter,summary_test_scatter,summary_test_with_train_scatter,summary_all_scatter]\n",
    "    for i in summary_list:\n",
    "        writer.add_summary(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe_and_output_dataset(name):\n",
    "    tesla_stocks = pd.read_csv('AdidasAG10Years.csv')\n",
    "    tesla_stocks.head()\n",
    "    data_to_use = tesla_stocks['close'].values\n",
    "    data_to_use = np.flipud(data_to_use) # flips values to get the newest values to the end\n",
    "    return data_to_use\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_dataset(min,max,dataset): #dataset shape: [x]\n",
    "    scaler = MinMaxScaler(feature_range=(-2,2))\n",
    "    scaler = scaler.fit(dataset)\n",
    "    scaled_dataset = scaler.transform(dataset)\n",
    "    return scaler, scaled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset_back(scaler, dataset): #dataset shape: [x]\n",
    "    return scaler.inverse_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if reshaped = False ->(None) to (None,1)\n",
    "#if reshaped = True -> (None,1) to (None)\n",
    "def reshape_dataset(dataset, reshaped):\n",
    "    if reshaped is False:\n",
    "        dataset = np.reshape(dataset, (-1,1))\n",
    "    else:\n",
    "        dataset = np.reshape(dataset, (-1))\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# is needed because output from Training is [batch_size,1] and all appended : [number_of_batches,batch_size,1]\n",
    "def unpack_predictions(prediction):\n",
    "    sup =[]\n",
    "    for i in range(len(prediction)):\n",
    "        for j in range(len(prediction[i])):\n",
    "            sup.append(prediction[i][j])\n",
    "    return sup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_predictions_test(tests):\n",
    "    test_results = []\n",
    "    for i in range(len(tests)):\n",
    "        for j in range(len(tests[i][0])):\n",
    "            test_results.append(tests[i][0][j])\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#requires dataset of shape (-1,1)\n",
    "def window_data(reshaped_data, window_size):\n",
    "    x = []\n",
    "    y = []    \n",
    "    i = 0\n",
    "    while (i + window_size) <= len(reshaped_data) - 1: #heiÃŸt eigentlich nach windowsSize Tagen = eine Prediction\n",
    "        x.append(reshaped_data[i:i+window_size])\n",
    "        y.append(reshaped_data[i+window_size])        \n",
    "        i += 1\n",
    "    assert len(x) ==  len(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires windowed_dataset\n",
    "def create_trains(windowed_data_x,windowed_data_y):\n",
    "    x_train  = np.array(windowed_data_x[:train_range])\n",
    "    y_train = np.array(windowed_data_y[:train_range])\n",
    "\n",
    "    x_test = np.array(windowed_data_x[train_range:])\n",
    "    y_test = np.array(windowed_data_y[train_range:])\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_cell_with_MultiRNN_Dropout(neurons_at_gate, batch_size,number_of_lstm_layers,dropout_rate):\n",
    "    layer_list = []\n",
    "    for counter in range(number_of_lstm_layers):\n",
    "        layer = tf.nn.rnn_cell.BasicLSTMCell(neurons_at_gate)\n",
    "        if dropout_rate != 0:\n",
    "            layer = tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n",
    "        layer_list.append(layer)     \n",
    "    cell = tf.contrib.rnn.MultiRNNCell(layer_list) \n",
    "    init_state = cell.zero_state(batch_size, tf.float32) #zero_state -> alle values to 0\n",
    "    return cell, init_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for given accuracy function\n",
    "#requires x shape of: \n",
    "def my_tf_round(x, decimal_place = 0):\n",
    "    multiplier = tf.constant(10**decimal_place, dtype=x.dtype)\n",
    "    return tf.round(x * multiplier) / multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires logits and labels shape of: (batch_size,feature_size), normal_case: (8,1)\n",
    "def accuracy(logits,labels,decimal_place):    \n",
    "    targets_rounded = my_tf_round(labels,decimal_place)\n",
    "    logits_rounded = my_tf_round(logits,decimal_place)\n",
    "    correct_prediction=tf.equal(logits_rounded,targets_rounded)\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_layer():\n",
    "    x_batch = tf.placeholder(tf.float32, [batch_size, window_size, 1], name='input_data')\n",
    "    y_batch = tf.placeholder(tf.float32, [batch_size, 1], name='targets')\n",
    "    return x_batch,y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rnn(x_batch,cell1,initial_state):\n",
    "    output_dynamic_cell,states = tf.nn.dynamic_rnn(cell1, x_batch, initial_state=initial_state)\n",
    "    return output_dynamic_cell, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lstm_output = (batch_size,window_size,neurons_at_gate)\n",
    "#number of classes = often 1\n",
    "#predictions_for_batch = (batch_size,number_of_features)\n",
    "def output_layer(lstm_output, neurons_at_gate, number_of_features):  \n",
    "    x = lstm_output[:, -1, :] \n",
    "    weights = tf.Variable(tf.truncated_normal([neurons_at_gate, number_of_features], stddev=0.05), name='output_layer_weights')\n",
    "    bias = tf.Variable(tf.zeros([number_of_features]), name='output_layer_bias')\n",
    "    predictions_for_batch = tf.matmul(x, weights) + bias \n",
    "    return predictions_for_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, targets): \n",
    "    losses = []\n",
    "    for i in range(targets.get_shape()[0]):\n",
    "        losses.append([(tf.pow(logits[i] - targets[i], 2))]) #mse      \n",
    "    loss = tf.reduce_sum(losses)/(2*batch_size)\n",
    "    return loss   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(loss,learning_rate,clip_on):\n",
    "    gradients = tf.gradients(loss, tf.trainable_variables(),name=\"gradientsCalculated\")\n",
    "    clipper_, _ = tf.clip_by_global_norm(gradients, clip_on,name=\"ClipperIsSetTo4\")\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate,name=\"AdamOptimizer\")\n",
    "    train_optimizer = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()),name=\"applyGradientsForOptimisation\")\n",
    "    return train_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(session,x_batch,y_batch):\n",
    "        logits_result,loss_result,accuracy_result,optimizer_result = session.run([logits,loss,accuracy,optimizer],feed_dict={x_batch_placeholder:x_batch,y_batch_placeholder:y_batch})\n",
    "        return logits_result,loss_result,accuracy_result,optimizer_result\n",
    "                                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_test(session,x_batch): #x_batch sollte eigentlich nur 1 window, mit dem man dann nicht nur 1 sondern x batches predicten kann\n",
    "    logits_test_result = session.run([logits],feed_dict={x_batch_placeholder:x_batch})\n",
    "    return logits_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_epochs(session,x_train,y_train,dataset,scaled_dataset,summary_writer):\n",
    "    \n",
    "    \n",
    "    epoch_loss = []\n",
    "    accuracy = 0.0\n",
    "    total_batch=0\n",
    "    traind_scores = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        epoch_loss = []\n",
    "        accuracy = 0.0\n",
    "        total_batch=0\n",
    "        traind_scores = [] \n",
    "        ii = 0\n",
    "        while(ii + batch_size) <= len(x_train): #ein Durchlauf = 1 batch = 1 BPTT = 1 Anpassung der Gewichte = 1x \"Lernen\"\n",
    "            x_batch = x_train[ii:ii+batch_size]\n",
    "            y_batch = y_train[ii:ii+batch_size]\n",
    "            logits,loss,accuracy,optimizer = run_batch(session,x_batch,y_batch)\n",
    "            total_batch+=1\n",
    "            ii += batch_size\n",
    "            \n",
    "            epoch_loss.append(loss)\n",
    "            traind_scores.append(logits)\n",
    "        if (i % 1) == 0:\n",
    "            print('Epoch {}/{}'.format((i+1), epochs), ' Current loss: {}'.format(np.mean(epoch_loss)))\n",
    "    \n",
    "    prediction_list = unpack_predictions(traind_scores) #reason: from [number_of_batches,batch_size,1] to [-1]\n",
    "    return prediction_list\n",
    "    #dataset = scale_dataset_back(scaler, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(session,x_test,summary_writer):  \n",
    "    tests = []\n",
    "    i = 0\n",
    "    while i+batch_size <= len(x_test): \n",
    "        logits_output = run_batch_test(session,x_test[i:i+batch_size])        \n",
    "        i += batch_size\n",
    "        tests.append(logits_output)\n",
    "        \n",
    "    result = unpack_predictions_test(tests)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(session):\n",
    "    summary_writer = tf.summary.FileWriter(logs_path_test,session.graph)\n",
    "    dataset=read_dataframe_and_output_dataset(\"AdidasAG10Years.csv\")\n",
    "    reshaped_data = reshape_dataset(dataset, False)\n",
    "    scaler, scaled_dataset = scale_dataset(-1,1,reshaped_data)\n",
    "    windowed_data_x, windowed_data_y = window_data(scaled_dataset, window_size)\n",
    "    x_train, y_train, x_test, y_test = create_trains(windowed_data_x,windowed_data_y)\n",
    "    \n",
    "    prediction_list = run_all_epochs(session,x_train,y_train,dataset,scaled_dataset,summary_writer)\n",
    "    result = test_network(session,x_test,summary_writer)\n",
    "    \n",
    "    create_and_add_summaries([dataset,scaled_dataset,prediction_list,result],writer=summary_writer,session=session) # if dataset changes, method have to be changed (updated)\n",
    "\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 7 #arbitrarily selectable value\n",
    "batch_size = 8\n",
    "epochs = 1 #eig 3\n",
    "train_range = 1600 #has to be dividable by batch_size\n",
    "dropout_rate=0.00\n",
    "number_of_lstm_layers= 1\n",
    "learning_rate=0.001\n",
    "neurons_at_gate=512\n",
    "logs_path_test= 'logs/'\n",
    "number_of_classes = 1 #ist glaube gleich mit number of features\n",
    "decimal_place = 1\n",
    "clip_on = 4\n",
    "#number_of_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_batch_placeholder,y_batch_placeholder = input_layer()\n",
    "cell, initial_state = create_lstm_cell_with_MultiRNN_Dropout(neurons_at_gate, batch_size, number_of_lstm_layers,dropout_rate)    \n",
    "outputs, states =  compute_rnn(x_batch_placeholder,cell,initial_state=initial_state)  \n",
    "logits = output_layer(outputs, neurons_at_gate, number_of_classes) \n",
    "loss = loss(logits,y_batch_placeholder)\n",
    "accuracy = accuracy(logits,y_batch_placeholder,decimal_place)\n",
    "optimizer= optimize(loss,learning_rate,clip_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Epoch 1/1  Current loss: 0.013013721443712711\n",
      "(912, 1)\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "print(\"start\")\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "run_all(sess)\n",
    "sess.close()\n",
    "print(\"finished\")\n",
    "###################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
